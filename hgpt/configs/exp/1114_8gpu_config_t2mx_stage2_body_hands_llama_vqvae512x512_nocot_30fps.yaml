SEED_VALUE: 1234 # Seed value
DEBUG: True # Debug mode

NAME: 1114-t2mx-nocot-t2m-meta-stage2-train-30fps # Experiment names
ACCELERATOR: 'gpu' # Devices optioncal: “cpu”, “gpu”, “tpu”, “ipu”, “hpu”, “mps, “auto”
NUM_NODES: 1 # Number of GPU nodes for distributed training
DEVICE: [0] # Index of gpus eg. [0] or [0,1,2,3] [0,1,2,3,4,5,6,7], "auto"

DATASET:
  target: hGPT.data.MotionX.MotionXDataModule
  NFEATS: 623
  WORD_VERTILIZER_PATH: deps/glove_t2m/
  TASK_PATH: 'templates/template_pretrain.json'
  MOTIONX:
    NAME: t2mx
    NJOINTS: 52
    DATA_ROOT: ""
    SPLIT_PATH: ""
    MOTION_FEAT_PATH: ""
    SEMANTIC_TEXT_PATH: ""
    COT_PATH: ""
    MEAN_STD_PATH: ""
    EVAL_MEAN_STD_PATH: ""
    MIN_MOTION_LEN: 60
    MAX_MOTION_LEN: 300
    UNIT_LEN: 4
    FRAME_RATE: 30.0
    VAE_WIN_SIZE: 64
    MOTION_TOKEN_PATH: ""
    MAX_TEXT_LEN: 20
    STD_TEXT: False


model:
  target: hGPT.models.hgpt.HumanoidGPT
  params:
    stage: ${TRAIN.STAGE}
    task: 't2m'
    lm: ${lm.llama}
    motion_vae: ${vq.vqvae_512_512}
    codebook_size: ${model.params.motion_vae.params.code_num}
    condition: 'text'
    metrics_dict: ${METRIC.TYPE}
    debug: ${DEBUG} 
    
TRAIN:
  #---------------------------------
  STAGE: lm_pretrain # stage "vae" , "lm_pretrain", "lm_instruct"
  SPLIT: 'train' # Training split name
  PRECISION: 'bf16-mixed' # PRECISION: 'bf16-mixed'
  STRATEGY: 'auto' # deepspeed_stage_2
  #---------------------------------
  NUM_WORKERS: 16 # Number of workers, 16 before
  BATCH_SIZE: 16 # Size of batches, 16 before
  ACCUMULATE_GRAD_BATCHES: 1 # 8 before
  END_EPOCH: 300 # End epoch
  RESUME: '' # Resume training from this path
  PRETRAINED: '' # Preatrained model path
  # PRETRAINED_VAE: 'vq/last.ckpt' # checkpoints/MotionGPT-base/motiongpt_s3_h3d.tar # Vae model path
  PRETRAINED_VAE: ''

  OPTIM:
    target: AdamW
    params:
      lr: 2e-4 # 2e-5 before
      betas: [0.9, 0.99]
      weight_decay: 0.0

  LR_SCHEDULER:
    target: CosineAnnealingLR
    params:
      T_max: ${eval:${LOGGER.VAL_EVERY_STEPS} * 100}
      eta_min: 1e-6

# Evaluating Configuration
EVAL:
  SPLIT: val
  BATCH_SIZE: 16 # Evaluating Batch size, 32 before
  NUM_WORKERS: 16 # Validation Batch size, 8 before

TEST:
  DATASETS: ['MotionX']
  #  'experiments/hgpt/0428-llama-nocot-train-padding-true/checkpoints/epoch=299.ckpt/checkpoint/mp_rank_00_model_states.pt'
  CHECKPOINTS: "" # checkpoints/MotionGPT-base/motiongpt_s3_h3d.tar

  SPLIT: 'test'  # test_small
  BATCH_SIZE: 32 # testing Batch size, 1 before
  NUM_WORKERS: 16

  SAVE_PREDICTIONS: False # Weather to save predictions
  COUNT_TIME: True # Weather to count time during test
  REPLICATION_TIMES: 3 # Number of times to replicate the test, 20 before， 1 for debug.
  REP_I: 0 # For counting replication times

LOSS:
  LAMBDA_FEATURE: 1.0
  LAMBDA_VELOCITY: 0.5
  LAMBDA_COMMIT: 0.02
  LAMBDA_CLS: 1.0
  ABLATION:
    RECONS_LOSS: 'l1_smooth'

METRIC:
  TYPE: ['TM2TMetrics']
  # TYPE: ['TM2TMetrics', 'PredMetrics', 'MMMetrics']
  FORCE_IN_METER: True
  DIST_SYNC_ON_STEP: True
  MM_NUM_SAMPLES: 100 # Number of samples for multimodal test
  MM_NUM_REPEATS: 30 # Number of repeats for multimodal test
  MM_NUM_TIMES: 10 # Number of times to repeat the multimodal test
  DIVERSITY_TIMES: 300 # Number of times to repeat the diversity test
  TM2T: ${eval.tm2t_x}

LOGGER:
  VAL_EVERY_STEPS: 100 # 10 before, 100 for debug.
  TYPE: ['wandb']
  WANDB:
    target: pytorch_lightning.loggers.WandbLogger
    params:
      project: hgpt
      offline: True
      id: null
      version: ''
      name: ${NAME}
      save_dir: ${FOLDER_EXP}
